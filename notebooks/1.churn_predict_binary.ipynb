{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7377a829",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2d91bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures, MinMaxScaler\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from src.models.features import CreditScoreTransformer, BalanceTransformer, AgeTransformer, RatioTransformer\n",
    "from src.models.evaluate import eval_PR, cv_confusion_matrix, confusion_matrix_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f7394c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore the specific warning about penalty=None\n",
    "warnings.filterwarnings('ignore', message=\"Setting penalty=None will ignore the C and l1_ratio parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d1a15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set directory paths\n",
    "project_root_dir = Path('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1304a55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load clean data and create train test split\n",
    "try:\n",
    "    train = pd.read_csv(project_root_dir/ 'data/clean/train.csv')\n",
    "    X = train.drop(['Exited'], axis=1)\n",
    "    y = train['Exited']\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")\n",
    "    print(\"Rerun previous notebook first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33922884",
   "metadata": {},
   "source": [
    "# Model Development\n",
    "In this section we will start building a predictive model for predicting customer churn. Due to the imbalanced ratio of the positive (churn) vs negative class (not churn), we will use average precision or AUC-PR as it is a more robust metric of performance in this case. In addition we will use 5-fold cross-validation when computing all of our metrics in order to reduce overfitting, in particular we will use the same folds for consistency. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24255b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "shared_cv = StratifiedKFold(shuffle=True, random_state=42)\n",
    "\n",
    "print(f\"Baseline churn rate: {100*y.mean():.2f}%\")\n",
    "\n",
    "models = []\n",
    "models_name = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ae8b7e",
   "metadata": {},
   "source": [
    "## Model 0 - Baseline\n",
    "- Logistic Regression Model with no regularization and class weights\n",
    "- Applied standardization for numerical features\n",
    "- Applied one hot encoding for categorical/binned features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d70966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline Logistic Regression Model\n",
    "model_0 = Pipeline([\n",
    "    (\n",
    "        'processor',\n",
    "        ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', StandardScaler(), \n",
    "                 ['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'EstimatedSalary']),\n",
    "                ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), \n",
    "                 ['Geography', 'Gender', 'HasCrCard', 'IsActiveMember'])\n",
    "            ]\n",
    "        )\n",
    "    ),\n",
    "    (\n",
    "        'classifier',\n",
    "        LogisticRegression(\n",
    "            C=np.inf, \n",
    "            class_weight='balanced', \n",
    "            solver='lbfgs', \n",
    "            max_iter=10000\n",
    "        )\n",
    "    )\n",
    "])\n",
    "\n",
    "models.append(model_0)\n",
    "models_name.append('Model 0')\n",
    "\n",
    "# Compute cross-validated confusion matrix and PR curve for baseline model\n",
    "display(cv_confusion_matrix(model_0, X, y, shared_cv))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d48069",
   "metadata": {},
   "source": [
    "## Model 0.1 - Baseline + distribution-aware processing\n",
    "From our exploratory data analysis we found that Age, Balance, CreditScore, and EstimatedSalary were not normally distributed. In particular we had that:\n",
    "- Age was right skewed with rounding spikes at multiples of 10\n",
    "- Balance had a zero-inflated normal distribution\n",
    "- CreditScore had a right censored normal distribution\n",
    "- EstimatedSalary was evenly distributed\n",
    "\n",
    "With this in mind we will perform the following techniques to account for these features:\n",
    "- Age: Flag spikes and log transform\n",
    "- Balance: Flag zero-balance and masked scale\n",
    "- CreditScore: Flag max and robust scale\n",
    "- EstimatedSalary: min-max scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28351777",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor_01 = ColumnTransformer(\n",
    "    [\n",
    "        # Numerical Features\n",
    "        ('Age', AgeTransformer(), ['Age']),\n",
    "        ('Balance', BalanceTransformer(), ['Balance']),\n",
    "        ('CreditScore', CreditScoreTransformer(), ['CreditScore']),\n",
    "        ('EstimatedSalary', MinMaxScaler(), ['EstimatedSalary']),\n",
    "        # Categorical Features\n",
    "        (\n",
    "            'OHEnc', \n",
    "            OneHotEncoder(dtype='int', sparse_output=False, drop='first'),\n",
    "            ['Geography', 'Gender', 'NumOfProducts']\n",
    "        ),\n",
    "        ('passthrough', 'passthrough', ['HasCrCard', 'IsActiveMember'])\n",
    "    ],\n",
    "    remainder='drop', verbose_feature_names_out=False\n",
    ")\n",
    "\n",
    "model_01 = Pipeline([\n",
    "    ('processor', processor_01),\n",
    "    ('classifier', LogisticRegression(C=np.inf, class_weight='balanced', solver='lbfgs', max_iter=10000))\n",
    "])\n",
    "\n",
    "models.append(model_01)\n",
    "models_name.append('Model 0.1')\n",
    "\n",
    "display(cv_confusion_matrix(model_01, X, y, shared_cv))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2901f38d",
   "metadata": {},
   "source": [
    "Cross-validated Metrics:\n",
    "Precision: 0.4523\n",
    "Recall: 0.7540\n",
    "F1 Score: 0.5654"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03dab04",
   "metadata": {},
   "source": [
    "## Model 0.1.1 - Baseline + distribution-aware processing + interaction features\n",
    "We now attempt to capture non-linear interactions between the features of the form feature_1xfeature_2. Furthermore we will use regularization in order to account for the resulting increase in the number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0608e467",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor_011 = Pipeline([\n",
    "    ('processor', processor_01),\n",
    "    ('interactions', PolynomialFeatures(degree=2, include_bias=False))\n",
    "])\n",
    "\n",
    "model_011 = Pipeline([\n",
    "    ('processor', processor_011),\n",
    "    ('classifier', LogisticRegression(\n",
    "        C=1,\n",
    "        l1_ratio = 1,\n",
    "        solver='liblinear',\n",
    "        class_weight='balanced',\n",
    "        max_iter=1000,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "models.append(model_011)\n",
    "models_name.append('Model 0.1.1')\n",
    "\n",
    "display(cv_confusion_matrix(model_011, X, y, shared_cv))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8208368e",
   "metadata": {},
   "source": [
    "## Model 1 - Tree Model\n",
    "- Gradient Boosted Trees\n",
    "- Apply one hot encoding to to categorical features (Geography, Gender)\n",
    "- No scaling of numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6b2f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor_1 = ColumnTransformer(\n",
    "    [\n",
    "        (\n",
    "            'OHEnc', \n",
    "            OneHotEncoder(dtype='int', sparse_output=False, drop='first'),\n",
    "            ['Geography', 'Gender']\n",
    "        )\n",
    "    ],\n",
    "    remainder='passthrough', verbose_feature_names_out=False\n",
    ")\n",
    "\n",
    "model_1 = Pipeline([\n",
    "    ('processor', processor_1),\n",
    "    ('classifier', XGBClassifier(\n",
    "        n_estimators = 1000,\n",
    "        max_depth = 4,\n",
    "        learning_rate = 0.05,\n",
    "        scale_pos_weight=4,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "\n",
    "    ))\n",
    "])\n",
    "\n",
    "models.append(model_1)\n",
    "models_name.append('Model 1')\n",
    "\n",
    "display(cv_confusion_matrix(model_1, X, y, shared_cv))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba2e98e",
   "metadata": {},
   "source": [
    "## Model 1.1 - Tree Model + Ratio Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9879a66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor_11 = ColumnTransformer(\n",
    "    [\n",
    "        ('OHEnc', OneHotEncoder(dtype='int', sparse_output=False, drop='first'), ['Geography', 'Gender']),\n",
    "        ('ratio', RatioTransformer(), ['CreditScore', 'Age', 'EstimatedSalary', 'Balance'])\n",
    "    ], \n",
    "    remainder='passthrough', verbose_feature_names_out=False\n",
    ")\n",
    "\n",
    "model_11 = Pipeline([\n",
    "    ('processor', processor_11),\n",
    "    ('classifier', XGBClassifier(\n",
    "        n_estimators = 1000,\n",
    "        max_depth = 4,\n",
    "        learning_rate = 0.05,\n",
    "        scale_pos_weight=4,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "    ))\n",
    "])\n",
    "\n",
    "models.append(model_11)\n",
    "models_name.append(\"Model 1.1\")\n",
    "\n",
    "display(cv_confusion_matrix(model_11, X, y, shared_cv))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11664910",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584dabf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Performance\n",
    "eval_PR(X, y, pipeline=models, cv=shared_cv, name=models_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce3b4c8",
   "metadata": {},
   "source": [
    "We choose Model 1 (Baseline Tree) as our final model as it had the best performance out of all the other iterations. In addition the best linear model is no longer an easily explainable model due to the complex feature engineering and hence should not be chosen when it has an approximately 4% drop in predictive power.\n",
    "\n",
    "Furthermore we will tune the model's hyperparameters in order to squeeze out some performance improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a21528e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune hyperparameters\n",
    "param_grid = {\n",
    "    'classifier__learning_rate': [0.01, 0.1, 0.2],\n",
    "    'classifier__n_estimators': [100, 200, 300],\n",
    "    'classifier__max_depth': [3, 5, 7],\n",
    "    'classifier__subsample': [0.8, 1.0],\n",
    "    'classifier__scale_pos_weight': [1, 2, 3, 4]\n",
    "}\n",
    "\n",
    "model_grid = GridSearchCV(\n",
    "    estimator=model_1,\n",
    "    param_grid=param_grid,\n",
    "    cv=shared_cv,\n",
    "    scoring='average_precision',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "model_grid.fit(X, y)\n",
    "\n",
    "display(model_grid.best_params_)\n",
    "final_model = model_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad9e05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "models.append(final_model)\n",
    "models_name.append('Final Model (Tuned)')\n",
    "\n",
    "display(cv_confusion_matrix(final_model, X, y, shared_cv))\n",
    "eval_PR(X, y, [model_1, final_model], shared_cv, name=['Final (Untuned)', 'Final (Tuned)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227c0ac4",
   "metadata": {},
   "source": [
    "# Final Model Evaluation\n",
    "In this section we will evaluate the performance of this model on our test set to estimate the model's performance on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3b4288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Test set and predict classes and probabilities.\n",
    "test = pd.read_csv(project_root_dir / 'data/clean/test.csv')\n",
    "X_test, y_test = test.drop(['Exited'], axis=1), test['Exited']\n",
    "\n",
    "y_pred = final_model.predict(X_test)\n",
    "y_pred_proba = final_model.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd444bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(confusion_matrix_df(y_test, y_pred))\n",
    "eval_PR(X, y, [final_model], shared_cv, name=['Final Model (CV Train)'],\n",
    "        final_pipeline=final_model, X_final=X_test, y_final=y_test, name_final='Final Model (Test)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6b371f",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "- Our final model performed better on the test set compared to when we estimated its performance using cross-validation on the training set.\n",
    "- From the Precision-Recall curve plot, we see that our model consistently outperforms all the other models at almost all levels of recall.\n",
    "- In addition we see that our model perfectly predicts the top ~15% of churners with 100% precision. Furthermore we do not start seeing a large trade-off between recall and precision until we reach a recall of ~37.5% in which case we are still accurately predicting churners at a rate of ~90%."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bank-churn-3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
